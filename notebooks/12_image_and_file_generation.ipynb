{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Lesson 12: Image & File Generation\n",
    "\n",
    "ðŸ”´ **Advanced** Â· â± **25 min**\n",
    "\n",
    "---\n",
    "\n",
    "lyzr-adk agents can generate images using DALL-E 3 or Gemini's image models, and produce file artifacts like CSVs, reports, and code files. This lesson shows how to configure image models and prompt your agent for visual and file outputs.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- Configure DALL-E 3 and Gemini image models on an agent using `set_image_model()`\n",
    "- Prompt an agent to generate images and interpret the response format\n",
    "- Understand how image URLs and artifact references are returned\n",
    "- Use agents to generate file artifacts like CSVs and structured reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "> **Note:** This is an **optional advanced lesson**. It is not required to complete the core lyzr-adk series.\n",
    "\n",
    "Before starting this lesson, you should have completed (or be familiar with):\n",
    "\n",
    "- **Lesson 1** â€” Getting Started (agent creation basics)\n",
    "- **Lesson 2** â€” Providers and Models\n",
    "- **Lesson 3** â€” Agent Lifecycle\n",
    "- **Lesson 4** â€” Structured Outputs\n",
    "\n",
    "You will also need:\n",
    "- A valid `LYZR_API_KEY` set as an environment variable (or replace the placeholder in the setup cell)\n",
    "- An **OpenAI API key** (for DALL-E 3) or a **Google API key** (for Gemini image models) linked to your Lyzr account, depending on which image model you choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lyzr-adk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lyzr import Studio\n",
    "\n",
    "API_KEY = os.getenv(\"LYZR_API_KEY\", \"YOUR_LYZR_API_KEY\")\n",
    "studio = Studio(api_key=API_KEY)\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "## Image Generation in lyzr-adk\n",
    "\n",
    "By default, a lyzr-adk agent only generates text. To enable image generation, you call `set_image_model()` on the agent after creating it. This tells the agent which image backend to use when it encounters a request to generate an image.\n",
    "\n",
    "Three image models are supported:\n",
    "\n",
    "| Model | String | Provider | Best for |\n",
    "|-------|--------|----------|---------|\n",
    "| DALL-E 3 | `\"dalle-3\"` | OpenAI | Photo-realistic, detailed |\n",
    "| Gemini Flash | `\"gemini-flash\"` | Google | Fast generation |\n",
    "| Gemini Pro | `\"gemini-pro\"` | Google | High quality |\n",
    "\n",
    "The `set_image_model()` call can be made at any time before you ask the agent to generate an image. You can also switch models between calls by calling it again with a different string.\n",
    "\n",
    "```python\n",
    "agent.set_image_model(\"dalle-3\")       # DALL-E 3 via OpenAI\n",
    "agent.set_image_model(\"gemini-flash\")  # Gemini Flash image gen\n",
    "agent.set_image_model(\"gemini-pro\")    # Gemini Pro image gen\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-06",
   "metadata": {},
   "source": [
    "## Setting Up an Image Generation Agent with DALL-E 3\n",
    "\n",
    "Create an agent as normal, then call `set_image_model()` to enable image generation. The underlying LLM (the `provider`) handles reasoning and prompt interpretation; the image model handles the actual image synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent and configure it with DALL-E 3\n",
    "image_agent = studio.create_agent(\n",
    "    name=\"Visual Creator\",\n",
    "    provider=\"openai/gpt-4o\",\n",
    "    role=\"Creative visual designer\",\n",
    "    goal=\"Generate high-quality images based on user descriptions\",\n",
    "    instructions=(\n",
    "        \"When asked to create or generate an image, produce a detailed, vivid image. \"\n",
    "        \"Always confirm what image you're generating before creating it.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the image model\n",
    "image_agent.set_image_model(\"dalle-3\")\n",
    "print(f\"âœ… Agent configured with DALL-E 3\")\n",
    "print(f\"   Agent ID: {image_agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "## Generating Your First Image\n",
    "\n",
    "Call `agent.run()` with a descriptive image prompt. The response object works the same as a text response â€” check `response.response` for the agent's message, and also inspect `response.image_url` or `response.artifact` for the generated image reference.\n",
    "\n",
    "The exact attribute name depends on the SDK version, so the cell below checks both possibilities and falls back to inspecting all available attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image\n",
    "response = image_agent.run(\n",
    "    \"Generate an image of a cozy coffee shop on a rainy evening, \"\n",
    "    \"with warm lighting, wooden tables, and people reading books.\"\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.response}\")\n",
    "\n",
    "# The response may contain an image URL or artifact reference\n",
    "# Depending on the SDK version, check response attributes\n",
    "if hasattr(response, 'image_url') and response.image_url:\n",
    "    print(f\"\\nImage URL: {response.image_url}\")\n",
    "elif hasattr(response, 'artifact') and response.artifact:\n",
    "    print(f\"\\nArtifact: {response.artifact}\")\n",
    "else:\n",
    "    print(\"\\n(Check response object for image data)\")\n",
    "    print(f\"Response type: {type(response)}\")\n",
    "    print(f\"Response attributes: {[a for a in dir(response) if not a.startswith('_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Switching to Gemini Image Models\n",
    "\n",
    "You are not locked into one image model per agent. Call `set_image_model()` again at any time to switch backends. You can also create separate agents for each model to run them in parallel.\n",
    "\n",
    "Use **Gemini Flash** when generation speed is the priority, and **Gemini Pro** when you need higher output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to Gemini Flash for faster generation\n",
    "gemini_agent = studio.create_agent(\n",
    "    name=\"Gemini Visual Creator\",\n",
    "    provider=\"google/gemini-2.0-flash\",\n",
    "    role=\"Creative visual designer\",\n",
    "    goal=\"Generate images quickly using Gemini\",\n",
    "    instructions=\"Generate creative, high-quality images based on descriptions.\"\n",
    ")\n",
    "\n",
    "gemini_agent.set_image_model(\"gemini-flash\")\n",
    "print(\"âœ… Agent configured with Gemini Flash\")\n",
    "\n",
    "response2 = gemini_agent.run(\n",
    "    \"Create an image of a robot learning to paint in an art studio, \"\n",
    "    \"surrounded by canvases and with paint on its hands.\"\n",
    ")\n",
    "print(f\"Response: {response2.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## File Generation\n",
    "\n",
    "Beyond images, agents can produce structured file outputs â€” CSVs, Markdown reports, JSON data, code files, and more. File generation does not require a special model configuration like `set_image_model()`. Instead, you prompt the agent to create the file content, and the response is returned as a text artifact that you can write to disk or process programmatically.\n",
    "\n",
    "Good system instructions for file-generating agents should:\n",
    "- Specify the expected output format (CSV, JSON, Markdown, etc.)\n",
    "- Indicate how many rows or sections to include\n",
    "- Ask for realistic or representative data rather than placeholder values\n",
    "\n",
    "The agent uses its LLM to reason about the request and produce well-formed file contents, which are returned in `response.response` or a dedicated `response.artifact` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent for file generation\n",
    "file_agent = studio.create_agent(\n",
    "    name=\"Report Generator\",\n",
    "    provider=\"openai/gpt-4o\",\n",
    "    role=\"Data analyst and report writer\",\n",
    "    goal=\"Generate structured data files and reports\",\n",
    "    instructions=(\n",
    "        \"When asked to create a file or report, generate complete, realistic data. \"\n",
    "        \"For CSV files, include headers and at least 5-10 rows of data. \"\n",
    "        \"For reports, be concise and use clear sections.\"\n",
    "    )\n",
    ")\n",
    "print(f\"âœ… File generation agent created: {file_agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask agent to generate a CSV report\n",
    "response3 = file_agent.run(\n",
    "    \"Generate a CSV file of Q1 2026 sales data with columns: \"\n",
    "    \"Date, Product, Region, Units_Sold, Revenue_USD. Include 10 rows.\"\n",
    ")\n",
    "\n",
    "print(f\"Response: {response3.response}\")\n",
    "\n",
    "# Check for file artifact\n",
    "if hasattr(response3, 'artifact') and response3.artifact:\n",
    "    print(f\"\\nFile artifact: {response3.artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Prompting Tips for Better Image Generation\n",
    "\n",
    "Image generation models respond strongly to the specificity and structure of the prompt. Vague prompts produce generic results; detailed prompts with clear style, mood, and composition guidance produce much better outputs.\n",
    "\n",
    "A well-structured image prompt typically includes:\n",
    "\n",
    "| Element | Example |\n",
    "|---------|----------|\n",
    "| **Subject** | \"a golden retriever dog\" |\n",
    "| **Setting** | \"in a sunlit park\" |\n",
    "| **Style** | \"watercolor painting\" |\n",
    "| **Lighting** | \"soft morning light\" |\n",
    "| **Mood** | \"peaceful and warm\" |\n",
    "| **Perspective** | \"close-up portrait\" |\n",
    "| **Quality hint** | \"highly detailed, professional illustration\" |\n",
    "\n",
    "You do not need all elements in every prompt, but including at least 3-4 of them consistently improves results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt engineering for image quality\n",
    "prompts = [\n",
    "    # Vague (less effective)\n",
    "    \"A city\",\n",
    "\n",
    "    # Specific (more effective)\n",
    "    \"A futuristic Tokyo skyline at dusk, cyberpunk aesthetic, \"\n",
    "    \"neon lights reflecting on wet streets, highly detailed, cinematic lighting\",\n",
    "\n",
    "    # With style guidance\n",
    "    \"Portrait of a golden retriever dog, watercolor painting style, \"\n",
    "    \"soft pastel colors, white background, professional illustration\"\n",
    "]\n",
    "\n",
    "print(\"Prompt engineering examples:\\n\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Prompt {i}: {prompt[:80]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… More specific prompts = better image results\")\n",
    "print(\"   Include: subject, setting, style, lighting, mood, perspective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Common Mistake: Not Calling `set_image_model()` Before Asking for Images\n",
    "\n",
    "If you ask an agent to generate an image without first calling `set_image_model()`, the agent will not have an image backend configured. Instead of synthesizing an image, it will fall back to describing the image in text â€” which is not the same thing.\n",
    "\n",
    "**Rule:** Always call `agent.set_image_model(\"dalle-3\")` (or another supported model string) before issuing image generation prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without set_image_model, agent will describe the image rather than generate it\n",
    "no_image_agent = studio.create_agent(\n",
    "    name=\"No Image Config\",\n",
    "    provider=\"openai/gpt-4o\",\n",
    "    role=\"Assistant\",\n",
    "    goal=\"Help\",\n",
    "    instructions=\"Help users.\"\n",
    ")\n",
    "# set_image_model() is intentionally NOT called here\n",
    "response4 = no_image_agent.run(\"Generate an image of a sunset over the ocean.\")\n",
    "print(f\"Without image model configured:\\n{response4.response[:200]}\")\n",
    "print(\"\\n(Agent will describe, not generate, without set_image_model)\")\n",
    "\n",
    "print(\"\\nâœ… Always call agent.set_image_model('dalle-3') before asking for images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Exercise: Generate and Compare Two Images\n",
    "\n",
    "Put it all together. Your task is to:\n",
    "\n",
    "1. Create an image generation agent with a model of your choice\n",
    "2. Configure it with `set_image_model()`\n",
    "3. Generate **Image 1** â€” a realistic scene with specific details\n",
    "4. Generate **Image 2** â€” the same subject but with an explicit artistic style (watercolor, oil painting, sketch, etc.)\n",
    "5. Print both prompts side by side to compare your approach\n",
    "\n",
    "Fill in the `TODO` sections in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an image generation agent\n",
    "my_image_agent = studio.create_agent(\n",
    "    name=...,\n",
    "    provider=...,  # openai/gpt-4o for DALL-E or google/gemini-2.0-flash for Gemini\n",
    "    role=...,\n",
    "    goal=...,\n",
    "    instructions=...\n",
    ")\n",
    "\n",
    "# TODO: Configure the image model\n",
    "my_image_agent.set_image_model(...)  # \"dalle-3\", \"gemini-flash\", or \"gemini-pro\"\n",
    "\n",
    "# TODO: Generate Image 1 â€” a realistic scene\n",
    "prompt_1 = \"...\"  # Describe a realistic scene with specific details\n",
    "response_1 = my_image_agent.run(prompt_1)\n",
    "print(f\"Image 1 response: {response_1.response[:200]}\")\n",
    "\n",
    "# TODO: Generate Image 2 â€” an artistic/stylized scene\n",
    "prompt_2 = \"...\"  # Same subject but with an artistic style (watercolor, oil painting, etc.)\n",
    "response_2 = my_image_agent.run(prompt_2)\n",
    "print(f\"\\nImage 2 response: {response_2.response[:200]}\")\n",
    "\n",
    "# TODO: Print both prompts side by side for comparison\n",
    "print(f\"\\nPrompt 1: {prompt_1}\")\n",
    "print(f\"Prompt 2: {prompt_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Image models at a glance\n",
    "\n",
    "| Model | String | Provider | Strength |\n",
    "|-------|--------|----------|---------|\n",
    "| DALL-E 3 | `\"dalle-3\"` | OpenAI | Photo-realistic, fine detail |\n",
    "| Gemini Flash | `\"gemini-flash\"` | Google | Fastest generation |\n",
    "| Gemini Pro | `\"gemini-pro\"` | Google | Highest quality from Google |\n",
    "\n",
    "### File generation capabilities\n",
    "\n",
    "| File type | How to prompt |\n",
    "|-----------|---------------|\n",
    "| CSV | \"Generate a CSV with columns X, Y, Z and 10 rows of data\" |\n",
    "| Markdown report | \"Write a report with sections: Overview, Data, Conclusion\" |\n",
    "| JSON | \"Return a JSON object with fields A, B, C\" |\n",
    "| Code file | \"Write a Python script that does X\" |\n",
    "\n",
    "### Prompt engineering tips\n",
    "\n",
    "- Include subject, setting, style, lighting, mood, and perspective\n",
    "- More specific = better results; vague prompts produce generic output\n",
    "- Name an artistic style explicitly (\"watercolor\", \"oil painting\", \"cinematic\") to guide aesthetics\n",
    "- For file generation, specify the schema, row count, and data realism in your prompt\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- `agent.set_image_model(\"dalle-3\")` must be called before asking for images â€” without it, the agent describes instead of generating\n",
    "- The same agent can switch image models at any time by calling `set_image_model()` again\n",
    "- Image output appears in `response.image_url` or `response.artifact` depending on the SDK version\n",
    "- File artifacts are returned in `response.response` or `response.artifact` and can be written to disk directly\n",
    "- Image generation and file generation are compatible with memory, sessions, and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You have completed Lesson 12. From here you can:\n",
    "\n",
    "- **Lesson 13: Advanced Features** â€” Reflection, Bias Check, and Groundedness for production-grade agent quality\n",
    "- **Back to Lesson 10: Capstone Project** â€” if you haven't completed it yet, the capstone brings together memory, tools, RAI, and knowledge bases into a full end-to-end build\n",
    "- **Back to Lesson 11: Streaming** â€” real-time token-by-token output for chat interfaces and CLIs\n",
    "\n",
    "---\n",
    "\n",
    "| Lesson | Topic |\n",
    "|--------|-------|\n",
    "| [01](./01_getting_started.ipynb) | Getting Started |\n",
    "| [02](./02_providers_and_models.ipynb) | Providers and Models |\n",
    "| [03](./03_agent_lifecycle.ipynb) | Agent Lifecycle |\n",
    "| [04](./04_structured_outputs.ipynb) | Structured Outputs |\n",
    "| [05](./05_memory_and_sessions.ipynb) | Memory and Sessions |\n",
    "| [06](./06_tools_and_functions.ipynb) | Tools and Functions |\n",
    "| [07](./07_knowledge_bases_rag.ipynb) | Knowledge Bases (RAG) |\n",
    "| [08](./08_contexts.ipynb) | Contexts |\n",
    "| [09](./09_rai_guardrails.ipynb) | RAI Guardrails |\n",
    "| [10](./10_capstone_project.ipynb) | Capstone Project |\n",
    "| [11](./11_streaming.ipynb) | Streaming |\n",
    "| **12** | **Image & File Generation (this lesson)** |"
   ]
  }
 ]
}
