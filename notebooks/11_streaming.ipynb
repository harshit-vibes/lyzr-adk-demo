{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# Lesson 11: Real-Time Streaming Responses\n",
    "\n",
    "üî¥ **Advanced** ¬∑ ‚è± **20 min**\n",
    "\n",
    "---\n",
    "\n",
    "By default, lyzr-adk waits for the complete response before returning it. Streaming changes this ‚Äî you receive tokens as they're generated, enabling real-time output in chatbots, CLIs, and interactive applications. This lesson covers how streaming works, when to use it, and its constraints.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- Understand the difference between streaming and non-streaming responses\n",
    "- Iterate over response chunks as they arrive in real time\n",
    "- Build a real-time streaming display and CLI-style chat loop\n",
    "- Understand which features are incompatible with streaming (RAI guardrails, structured outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "> **Note:** This is an **optional advanced lesson**. It is not required to complete the core lyzr-adk series.\n",
    "\n",
    "Before starting this lesson, you should have completed (or be familiar with):\n",
    "\n",
    "- **Lesson 1** ‚Äî Getting Started (agent creation basics)\n",
    "- **Lesson 2** ‚Äî Providers and Models\n",
    "- **Lesson 3** ‚Äî Agent Lifecycle\n",
    "- **Lesson 4** ‚Äî Structured Outputs\n",
    "- **Lesson 5** ‚Äî Memory and Sessions\n",
    "\n",
    "You will also need:\n",
    "- A valid `LYZR_API_KEY` set as an environment variable (or replace the placeholder in the setup cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lyzr-adk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from lyzr import Studio\n",
    "\n",
    "API_KEY = os.getenv(\"LYZR_API_KEY\", \"YOUR_LYZR_API_KEY\")\n",
    "studio = Studio(api_key=API_KEY)\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "## Streaming vs Non-Streaming\n",
    "\n",
    "Understanding the trade-offs between the two modes helps you choose the right one for each situation.\n",
    "\n",
    "| | Non-Streaming (default) | Streaming |\n",
    "|---|---|---|\n",
    "| **How it works** | Waits for complete response | Yields chunks as generated |\n",
    "| **Latency to first token** | High (wait for all) | Very low |\n",
    "| **Use case** | Batch processing, structured data | Chat UIs, CLIs, live display |\n",
    "| **Response type** | `response.response` (string) | Iterator of string chunks |\n",
    "| **With RAI?** | ‚úÖ Yes | ‚ùå No |\n",
    "| **Structured output?** | ‚úÖ Yes | ‚ùå No |\n",
    "\n",
    "The key API difference is a single argument: `stream=True` vs `stream=False` (default).\n",
    "\n",
    "```python\n",
    "# Non-streaming (default)\n",
    "response = agent.run(\"message\", stream=False)\n",
    "print(response.response)          # full string\n",
    "\n",
    "# Streaming\n",
    "for chunk in agent.run(\"message\", stream=True):\n",
    "    print(chunk, end=\"\", flush=True)  # chunk is a string fragment\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-06",
   "metadata": {},
   "source": [
    "## Creating an Agent for Streaming\n",
    "\n",
    "Agent creation is identical for streaming and non-streaming. The `stream` parameter is only passed to `agent.run()`, not to `create_agent()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent ‚Äî same as always\n",
    "stream_agent = studio.create_agent(\n",
    "    name=\"Stream Demo Agent\",\n",
    "    provider=\"openai/gpt-4o\",\n",
    "    role=\"Storyteller and explainer\",\n",
    "    goal=\"Give detailed, engaging responses\",\n",
    "    instructions=\"Be thorough and descriptive. Use complete sentences.\"\n",
    ")\n",
    "print(f\"Agent created: {stream_agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-08",
   "metadata": {},
   "source": [
    "## Your First Streaming Response\n",
    "\n",
    "Pass `stream=True` to `agent.run()` and iterate over the result. Each iteration yields a string chunk ‚Äî a fragment of the response as it is generated.\n",
    "\n",
    "Two important details for real-time display:\n",
    "- `end=\"\"` prevents `print` from adding a newline after each chunk\n",
    "- `flush=True` forces the output buffer to flush immediately so tokens appear as they arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response (tokens appear as they arrive):\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# stream=True returns an iterator of string chunks\n",
    "for chunk in stream_agent.run(\"Explain how neural networks learn in simple terms.\", stream=True):\n",
    "    print(chunk, end=\"\", flush=True)  # flush=True ensures immediate display\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"\\n‚úÖ Stream complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Collecting Streamed Chunks\n",
    "\n",
    "Sometimes you want to display chunks in real time *and* have the complete response available afterward ‚Äî for logging, post-processing, or analysis. Simply accumulate chunks in a list and join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all chunks to reconstruct the full response\n",
    "chunks = []\n",
    "print(\"Streaming and collecting:\\n\")\n",
    "\n",
    "for chunk in stream_agent.run(\"What are the three laws of robotics?\", stream=True):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Reconstruct the full text\n",
    "full_response = \"\".join(chunks)\n",
    "word_count = len(full_response.split())\n",
    "print(f\"\\nüìä Stats: {len(chunks)} chunks received, {word_count} words total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Streaming with Sessions and Memory\n",
    "\n",
    "Streaming is fully compatible with memory and sessions. You can pass `session_id` to `agent.run()` exactly as you would in non-streaming mode ‚Äî the agent maintains conversation context across turns.\n",
    "\n",
    "This makes streaming suitable for real-time chat interfaces where continuity across turns is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "stream_agent.add_memory(max_messages=10)\n",
    "session = str(uuid.uuid4())\n",
    "\n",
    "# Turn 1: stream with session\n",
    "print(\"Turn 1:\")\n",
    "for chunk in stream_agent.run(\"My name is Alex and I love astronomy.\", stream=True, session_id=session):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Turn 2: stream followup ‚Äî agent remembers\n",
    "print(\"Turn 2 (agent should remember Alex):\")\n",
    "for chunk in stream_agent.run(\"What's my name and what do I love?\", stream=True, session_id=session):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Comparing Speed: Non-Streaming vs Streaming\n",
    "\n",
    "The total time to receive the complete response is roughly the same in both modes ‚Äî the LLM generates the same number of tokens either way. The meaningful difference is **time to first token**:\n",
    "\n",
    "- **Non-streaming**: you wait until the entire response is generated before seeing anything\n",
    "- **Streaming**: the first token appears almost immediately, making the interaction feel much faster to the user\n",
    "\n",
    "This perceived responsiveness is the primary reason to use streaming in interactive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"List 5 interesting facts about black holes.\"\n",
    "\n",
    "# Non-streaming: measure time to full response\n",
    "start = time.time()\n",
    "response = stream_agent.run(question, stream=False)\n",
    "total_time = time.time() - start\n",
    "print(f\"Non-streaming: {total_time:.2f}s to full response\")\n",
    "print(f\"Response: {response.response[:100]}...\\n\")\n",
    "\n",
    "# Streaming: measure time to first token\n",
    "start = time.time()\n",
    "first_token_time = None\n",
    "all_chunks = []\n",
    "for chunk in stream_agent.run(question, stream=True):\n",
    "    if first_token_time is None:\n",
    "        first_token_time = time.time() - start\n",
    "    all_chunks.append(chunk)\n",
    "print(f\"Streaming: {first_token_time:.2f}s to first token, {time.time()-start:.2f}s total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Common Mistakes: Incompatibilities with Streaming\n",
    "\n",
    "Two features are **incompatible** with streaming:\n",
    "\n",
    "### 1. Structured Outputs (`response_format`)\n",
    "Structured outputs require the complete response to be available before it can be parsed and validated against a schema. Streaming yields raw chunks, making schema validation impossible mid-stream.\n",
    "\n",
    "**Rule:** If you need a `response_format`, use `stream=False`.\n",
    "\n",
    "### 2. RAI Guardrails\n",
    "RAI (Responsible AI) policies inspect the full response content before returning it ‚Äî for toxicity checks, content filtering, and so on. This inspection step requires the complete response, which is unavailable during streaming.\n",
    "\n",
    "**Rule:** If you have `add_rai_policy()` on an agent, use `stream=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    text: str\n",
    "    word_count: int\n",
    "\n",
    "# ‚ùå Mistake 1: structured output with streaming\n",
    "try:\n",
    "    for chunk in stream_agent.run(\"Summarize AI\", stream=True, response_format=Summary):\n",
    "        print(chunk)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Structured output + streaming error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ‚ùå Mistake 2: RAI policy + streaming\n",
    "try:\n",
    "    policy = studio.create_rai_policy(name=\"Test\", toxicity=True)\n",
    "    rai_agent = studio.create_agent(\n",
    "        name=\"RAI Stream Test\", provider=\"openai/gpt-4o\",\n",
    "        role=\"Test\", goal=\"Test\", instructions=\"Test\"\n",
    "    )\n",
    "    rai_agent.add_rai_policy(policy)\n",
    "    for chunk in rai_agent.run(\"Hello\", stream=True):\n",
    "        print(chunk)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå RAI + streaming error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: use stream=False when using RAI or structured outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Exercise: Build a Streaming CLI Chat Loop\n",
    "\n",
    "Put it all together. Your task is to build a simple CLI-style chat loop that:\n",
    "\n",
    "1. Creates a conversational agent with memory enabled\n",
    "2. Starts a new session with a unique ID\n",
    "3. Reads user input with `input()`\n",
    "4. Streams the agent's response token by token\n",
    "5. Maintains conversation context across turns (via `session_id`)\n",
    "6. Exits cleanly when the user types `quit`, `exit`, or `q`\n",
    "\n",
    "Fill in the `TODO` sections in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# TODO: Create an agent suitable for chat\n",
    "chat_agent = studio.create_agent(\n",
    "    name=...,\n",
    "    provider=\"openai/gpt-4o\",\n",
    "    role=...,\n",
    "    goal=...,\n",
    "    instructions=...\n",
    ")\n",
    "chat_agent.add_memory(max_messages=20)\n",
    "\n",
    "# TODO: Start a chat session\n",
    "chat_session = str(uuid.uuid4())\n",
    "\n",
    "# Simple streaming chat loop\n",
    "print(\"Chat started! Type 'quit' to exit.\\n\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Chat ended.\")\n",
    "        break\n",
    "    if not user_input:\n",
    "        continue\n",
    "\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "    # TODO: Stream the response using stream=True and session_id=chat_session\n",
    "    ...\n",
    "    print()  # newline after each response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### When to use streaming vs non-streaming\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|---|---|\n",
    "| Chat UI or CLI with real-time output | `stream=True` |\n",
    "| Batch processing or automation | `stream=False` |\n",
    "| Structured output (`response_format`) | `stream=False` |\n",
    "| RAI guardrails (`add_rai_policy`) | `stream=False` |\n",
    "| Memory and sessions | Either ‚Äî both work |\n",
    "| Tools and function calling | Either ‚Äî both work |\n",
    "| Contexts and knowledge bases | Either ‚Äî both work |\n",
    "\n",
    "### Compatibility matrix\n",
    "\n",
    "| Feature | Non-Streaming | Streaming |\n",
    "|---|---|---|\n",
    "| Memory / sessions | ‚úÖ | ‚úÖ |\n",
    "| Tools / functions | ‚úÖ | ‚úÖ |\n",
    "| Contexts | ‚úÖ | ‚úÖ |\n",
    "| Knowledge bases | ‚úÖ | ‚úÖ |\n",
    "| Structured outputs | ‚úÖ | ‚ùå |\n",
    "| RAI guardrails | ‚úÖ | ‚ùå |\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- `agent.run(\"...\", stream=True)` returns an iterator of string chunks\n",
    "- Use `print(chunk, end=\"\", flush=True)` for real-time terminal display\n",
    "- Collect chunks into a list and `\"\".join(chunks)` to reconstruct the full response\n",
    "- Streaming dramatically reduces **time to first token**, improving perceived responsiveness\n",
    "- RAI guardrails and structured outputs require `stream=False`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You have completed Lesson 11. From here you can:\n",
    "\n",
    "- **Lesson 12: Image and File Generation** ‚Äî explore multimodal output capabilities (coming soon)\n",
    "- **Back to Lesson 10: Capstone Project** ‚Äî if you haven't completed it yet, the capstone brings together memory, tools, RAI, and knowledge bases into a full end-to-end build\n",
    "\n",
    "---\n",
    "\n",
    "| Lesson | Topic |\n",
    "|---|---|\n",
    "| [01](./01_getting_started.ipynb) | Getting Started |\n",
    "| [02](./02_providers_and_models.ipynb) | Providers and Models |\n",
    "| [03](./03_agent_lifecycle.ipynb) | Agent Lifecycle |\n",
    "| [04](./04_structured_outputs.ipynb) | Structured Outputs |\n",
    "| [05](./05_memory_and_sessions.ipynb) | Memory and Sessions |\n",
    "| [06](./06_tools_and_functions.ipynb) | Tools and Functions |\n",
    "| [07](./07_knowledge_bases_rag.ipynb) | Knowledge Bases (RAG) |\n",
    "| [08](./08_contexts.ipynb) | Contexts |\n",
    "| [09](./09_rai_guardrails.ipynb) | RAI Guardrails |\n",
    "| [10](./10_capstone_project.ipynb) | Capstone Project |\n",
    "| **11** | **Streaming (this lesson)** |"
   ]
  }
 ]
}
